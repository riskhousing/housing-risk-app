{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def stage(msg: str):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(msg)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def timed(msg: str):\n",
    "    # simple context manager-ish helper\n",
    "    class _T:\n",
    "        def __enter__(self):\n",
    "            self.t0 = time.time()\n",
    "            print(f\"▶ {msg} ...\")\n",
    "            return self\n",
    "        def __exit__(self, exc_type, exc, tb):\n",
    "            dt = time.time() - self.t0\n",
    "            if exc_type is None:\n",
    "                print(f\"✅ Done in {dt:.2f}s\")\n",
    "            else:\n",
    "                print(f\"❌ Failed after {dt:.2f}s\")\n",
    "    return _T()\n",
    "\n",
    "    stage(\"Stage 1/7 — Load dataset + verify columns\")\n",
    "\n",
    "DATA_PATH = \"data.csv\"\n",
    "TARGET_COL = \"risk\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"FAULT_DISTANCE\",\n",
    "    \"BASIC_WIND_SPEED\",\n",
    "    \"SLOPE\",\n",
    "    \"ELEVATION\",\n",
    "    \"POTENTIAL_LIQUEFACTION\",\n",
    "    \"DISTANCE_TO_RIVERS_AND_SEAS\",\n",
    "    \"SURFACE_RUN_OFF\",\n",
    "    \"VERTICAL_IRREGUARITY\",\n",
    "    \"BUILDING_PROXIMITY\",\n",
    "    \"NUMBER_OF_BAYS\",\n",
    "    \"COLUMN_SPACING\",\n",
    "    \"MAXIMUM_CRACK\",\n",
    "    \"ROOF_SLOPE\",\n",
    "    \"ROOF_DESIGN\",\n",
    "    \"ROOF_FASTENER_DISTANCE\",\n",
    "]\n",
    "\n",
    "with timed(f\"Reading {DATA_PATH}\"):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", len(df.columns))\n",
    "\n",
    "missing = [c for c in FEATURES + [TARGET_COL] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "df = df[FEATURES + [TARGET_COL]].copy()\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n",
    "df.head(3)\n",
    "\n",
    "stage(\"Stage 2/7 — Define feature types\")\n",
    "\n",
    "categorical_features = [\n",
    "    \"POTENTIAL_LIQUEFACTION\",\n",
    "    \"SURFACE_RUN_OFF\",\n",
    "    \"ROOF_DESIGN\",\n",
    "    \"VERTICAL_IRREGUARITY\",\n",
    "    \"BUILDING_PROXIMITY\",\n",
    "]\n",
    "\n",
    "numeric_features = [c for c in FEATURES if c not in categorical_features]\n",
    "\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "stage(\"Stage 3/7 — Build preprocessing pipeline\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✅ Preprocess pipeline created.\")\n",
    "\n",
    "stage(\"Stage 4/7 — Train/validation split\")\n",
    "\n",
    "with timed(\"Splitting train/val\"):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y if y.nunique() > 1 else None\n",
    "    )\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "print(\"\\nTrain target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nVal target distribution:\")\n",
    "print(y_val.value_counts())\n",
    "\n",
    "stage(\"Stage 5/7 — Train RandomForest (with progress)\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=150,        # prototype speed; increase later if needed\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,               # IMPORTANT: use all cores\n",
    "    verbose=1                # shows training progress in output\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "with timed(\"Fitting model (watch verbose output below)\"):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Training finished.\")\n",
    "\n",
    "stage(\"Stage 6/7 — Evaluate\")\n",
    "\n",
    "with timed(\"Predicting on validation set\"):\n",
    "    pred = clf.predict(X_val)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_val, pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_val, pred))\n",
    "\n",
    "stage(\"Stage 7/7 — Save model for FastAPI\")\n",
    "\n",
    "MODEL_OUT = \"model.joblib\"\n",
    "\n",
    "with timed(f\"Saving model to {MODEL_OUT}\"):\n",
    "    joblib.dump(clf, MODEL_OUT)\n",
    "\n",
    "print(f\"✅ Saved: {MODEL_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Stage 1/7 — Load dataset + verify columns\n",
      "================================================================================\n",
      "▶ Reading data.csv ...\n",
      "✅ Done in 0.02s\n",
      "Rows: 1000\n",
      "Target distribution:\n",
      "risk\n",
      "HIGH        338\n",
      "MODERATE    332\n",
      "LOW         330\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Stage 2/7 — Train/validation split\n",
      "================================================================================\n",
      "▶ Splitting train/val ...\n",
      "✅ Done in 0.01s\n",
      "Train size: 800\n",
      "Val size: 200\n",
      "\n",
      "================================================================================\n",
      "Stage 3/7 — Preprocess + model pipeline\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Stage 4/7 — Train\n",
      "================================================================================\n",
      "▶ Fitting model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 300 out of 300 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done in 0.98s\n",
      "\n",
      "================================================================================\n",
      "Stage 5/7 — Evaluate\n",
      "================================================================================\n",
      "▶ Predicting on validation set ...\n",
      "✅ Done in 0.10s\n",
      "Confusion matrix:\n",
      "[[68  0  0]\n",
      " [ 0 66  0]\n",
      " [ 0  0 66]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       1.00      1.00      1.00        68\n",
      "         LOW       1.00      1.00      1.00        66\n",
      "    MODERATE       1.00      1.00      1.00        66\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Stage 6/7 — Save model + schema\n",
      "================================================================================\n",
      "▶ Saving model to questionnaire_model.joblib ...\n",
      "✅ Done in 0.30s\n",
      "✅ Saved: questionnaire_model.joblib\n",
      "✅ Saved: questionnaire_schema.joblib\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def stage(msg: str):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(msg)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def timed(msg: str):\n",
    "    class _T:\n",
    "        def __enter__(self):\n",
    "            self.t0 = time.time()\n",
    "            print(f\"▶ {msg} ...\")\n",
    "            return self\n",
    "        def __exit__(self, exc_type, exc, tb):\n",
    "            dt = time.time() - self.t0\n",
    "            if exc_type is None:\n",
    "                print(f\"✅ Done in {dt:.2f}s\")\n",
    "            else:\n",
    "                print(f\"❌ Failed after {dt:.2f}s\")\n",
    "    return _T()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Questionnaire schema\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"data.csv\"\n",
    "\n",
    "# Pick ONE target column name used in your CSV:\n",
    "TARGET_COL = \"risk\"  # or \"RISK_DESCRIPTION\" etc.\n",
    "\n",
    "# All questionnaire item features (store values as integers 1/2/3)\n",
    "FEATURES = [\n",
    "    # HAZARD (A)\n",
    "    \"A1_1_PEIS\",\n",
    "    \"A1_2_FAULT_DISTANCE\",\n",
    "    \"A1_3_SEISMIC_SOURCE_TYPE\",\n",
    "    \"A1_4_LIQUEFACTION\",\n",
    "    \"A2_1_BASIC_WIND_SPEED\",\n",
    "    \"A2_2_BUILDING_VICINITY\",\n",
    "    \"A3_1_SLOPE\",\n",
    "    \"A3_2_ELEVATION\",\n",
    "    \"A3_3_DISTANCE_TO_RIVERS_AND_SEAS\",\n",
    "    \"A3_4_SURFACE_RUNOFF\",\n",
    "    \"A3_5_BASE_HEIGHT\",\n",
    "    \"A3_6_DRAINAGE_SYSTEM\",\n",
    "\n",
    "    # EXPOSURE (B)\n",
    "    \"B1_1_AESTHETIC_THEME\",\n",
    "    \"B1_2_STYLE_UNIQUE\",\n",
    "    \"B1_3_STYLE_TYPICAL\",\n",
    "    \"B1_4_CITYSCAPE_INTEGRATION\",\n",
    "    \"B2_1_AGE_OF_BUILDING\",\n",
    "    \"B2_2_PAST_RELEVANCE\",\n",
    "    \"B2_3_GEO_IMPACT\",\n",
    "    \"B2_4_CULTURAL_HERITAGE_TIE\",\n",
    "    \"B2_5_MESSAGE_WORTH_PRESERVING\",\n",
    "    \"B3_1_NO_INITIATIVES\",\n",
    "    \"B3_2_PROMINENT_SUPPORT\",\n",
    "    \"B3_3_IMPORTANCE_DAILY_LIFE\",\n",
    "    \"B3_4_NO_PROMOTION\",\n",
    "    \"B4_1_TOURIST_MUST_SEE\",\n",
    "    \"B4_2_TOURISM_CONTRIBUTION\",\n",
    "    \"B4_3_VISITED_FOR_GOODS\",\n",
    "    \"B4_4_CURRENT_USE_ADOPTS_NEEDS\",\n",
    "\n",
    "    # VULNERABILITY (C)\n",
    "    \"C1_1_CODE_YEAR_BUILT\",\n",
    "    \"C1_2_PLAN_IRREGULARITY\",\n",
    "    \"C1_3_VERTICAL_IRREGULARITY\",\n",
    "    \"C1_4_BUILDING_PROXIMITY\",\n",
    "    \"C1_5_NUMBER_OF_STOREYS\",\n",
    "    \"C1_6_STRUCT_SYSTEM_MATERIAL\",\n",
    "    \"C1_7_NUMBER_OF_BAYS\",\n",
    "    \"C1_8_COLUMN_SPACING\",\n",
    "    \"C1_9_BUILDING_ENCLOSURE\",\n",
    "    \"C1_10_WALL_MATERIAL\",\n",
    "    \"C1_11_FRAMING_TYPE\",\n",
    "    \"C1_12_FLOORING_MATERIAL\",\n",
    "    \"C2_1_CRACK_WIDTH\",\n",
    "    \"C2_2_UNEVEN_SETTLEMENT\",\n",
    "    \"C2_3_BEAM_COLUMN_DEFORMATION\",\n",
    "    \"C2_4_FINISHING_DETERIORATION\",\n",
    "    \"C2_5_MEMBER_DECAY\",\n",
    "    \"C2_6_ADDITIONAL_LOADS\",\n",
    "    \"C3_1_ROOF_DESIGN\",\n",
    "    \"C3_2_ROOF_SLOPE\",\n",
    "    \"C3_3_ROOFING_MATERIAL\",\n",
    "    \"C4_1_ROOF_FASTENERS\",\n",
    "    \"C4_2_FASTENER_SPACING\",\n",
    "]\n",
    "\n",
    "# Weights from your questionnaire table (the left numeric weight per item).\n",
    "# (These are constants used only to compute engineered totals.)\n",
    "WEIGHTS = {\n",
    "    # A - hazard\n",
    "    \"A1_1_PEIS\": 3,\n",
    "    \"A1_2_FAULT_DISTANCE\": 3,\n",
    "    \"A1_3_SEISMIC_SOURCE_TYPE\": 3,\n",
    "    \"A1_4_LIQUEFACTION\": 3,\n",
    "    \"A2_1_BASIC_WIND_SPEED\": 2,\n",
    "    \"A2_2_BUILDING_VICINITY\": 2,\n",
    "    \"A3_1_SLOPE\": 1,\n",
    "    \"A3_2_ELEVATION\": 1,\n",
    "    \"A3_3_DISTANCE_TO_RIVERS_AND_SEAS\": 3,\n",
    "    \"A3_4_SURFACE_RUNOFF\": 1,\n",
    "    \"A3_5_BASE_HEIGHT\": 1,\n",
    "    \"A3_6_DRAINAGE_SYSTEM\": 2,\n",
    "\n",
    "    # B - exposure (keep weights you’re using in the sheet)\n",
    "    \"B1_1_AESTHETIC_THEME\": 2,\n",
    "    \"B1_2_STYLE_UNIQUE\": 1,\n",
    "    \"B1_3_STYLE_TYPICAL\": 1,\n",
    "    \"B1_4_CITYSCAPE_INTEGRATION\": 2,\n",
    "    \"B2_1_AGE_OF_BUILDING\": 2,\n",
    "    \"B2_2_PAST_RELEVANCE\": 3,\n",
    "    \"B2_3_GEO_IMPACT\": 1,\n",
    "    \"B2_4_CULTURAL_HERITAGE_TIE\": 2,\n",
    "    \"B2_5_MESSAGE_WORTH_PRESERVING\": 2,\n",
    "    \"B3_1_NO_INITIATIVES\": 3,\n",
    "    \"B3_2_PROMINENT_SUPPORT\": 3,\n",
    "    \"B3_3_IMPORTANCE_DAILY_LIFE\": 2,\n",
    "    \"B3_4_NO_PROMOTION\": 3,\n",
    "    \"B4_1_TOURIST_MUST_SEE\": 2,\n",
    "    \"B4_2_TOURISM_CONTRIBUTION\": 1,\n",
    "    \"B4_3_VISITED_FOR_GOODS\": 1,\n",
    "    \"B4_4_CURRENT_USE_ADOPTS_NEEDS\": 2,\n",
    "\n",
    "    # C - vulnerability\n",
    "    \"C1_1_CODE_YEAR_BUILT\": 3,\n",
    "    \"C1_2_PLAN_IRREGULARITY\": 3,\n",
    "    \"C1_3_VERTICAL_IRREGULARITY\": 2,\n",
    "    \"C1_4_BUILDING_PROXIMITY\": 1,\n",
    "    \"C1_5_NUMBER_OF_STOREYS\": 2,\n",
    "    \"C1_6_STRUCT_SYSTEM_MATERIAL\": 1,\n",
    "    \"C1_7_NUMBER_OF_BAYS\": 3,\n",
    "    \"C1_8_COLUMN_SPACING\": 1,\n",
    "    \"C1_9_BUILDING_ENCLOSURE\": 3,\n",
    "    \"C1_10_WALL_MATERIAL\": 3,\n",
    "    \"C1_11_FRAMING_TYPE\": 3,\n",
    "    \"C1_12_FLOORING_MATERIAL\": 1,\n",
    "    \"C2_1_CRACK_WIDTH\": 2,\n",
    "    \"C2_2_UNEVEN_SETTLEMENT\": 1,\n",
    "    \"C2_3_BEAM_COLUMN_DEFORMATION\": 3,\n",
    "    \"C2_4_FINISHING_DETERIORATION\": 3,\n",
    "    \"C2_5_MEMBER_DECAY\": 3,\n",
    "    \"C2_6_ADDITIONAL_LOADS\": 1,\n",
    "    \"C3_1_ROOF_DESIGN\": 3,\n",
    "    \"C3_2_ROOF_SLOPE\": 3,\n",
    "    \"C3_3_ROOFING_MATERIAL\": 2,\n",
    "    \"C4_1_ROOF_FASTENERS\": 2,\n",
    "    \"C4_2_FASTENER_SPACING\": 2,\n",
    "}\n",
    "\n",
    "A_COLS = [c for c in FEATURES if c.startswith(\"A\")]\n",
    "B_COLS = [c for c in FEATURES if c.startswith(\"B\")]\n",
    "C_COLS = [c for c in FEATURES if c.startswith(\"C\")]\n",
    "\n",
    "def add_engineered_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Weighted totals\n",
    "    def wsum(cols):\n",
    "        return sum(df[c].astype(float) * WEIGHTS.get(c, 1) for c in cols)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"HAZARD_SCORE\"] = wsum(A_COLS)\n",
    "    df[\"EXPOSURE_SCORE\"] = wsum(B_COLS)\n",
    "    df[\"VULNERABILITY_SCORE\"] = wsum(C_COLS)\n",
    "\n",
    "    # Simple combined index (you can replace with your spreadsheet formula if you have one)\n",
    "    df[\"RISK_INDEX\"] = df[\"HAZARD_SCORE\"] + df[\"EXPOSURE_SCORE\"] + df[\"VULNERABILITY_SCORE\"]\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load + verify\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "stage(\"Stage 1/7 — Load dataset + verify columns\")\n",
    "\n",
    "with timed(f\"Reading {DATA_PATH}\"):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "missing = [c for c in FEATURES + [TARGET_COL] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "df = df[FEATURES + [TARGET_COL]].copy()\n",
    "\n",
    "# Ensure features are numeric 1/2/3 (strings -> numeric)\n",
    "for c in FEATURES:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Add engineered totals (hazard/exposure/vulnerability + risk index)\n",
    "df = add_engineered_scores(df)\n",
    "\n",
    "ALL_FEATURES = FEATURES + [\"HAZARD_SCORE\", \"EXPOSURE_SCORE\", \"VULNERABILITY_SCORE\", \"RISK_INDEX\"]\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Target distribution:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Train/val split\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "stage(\"Stage 2/7 — Train/validation split\")\n",
    "\n",
    "X = df[ALL_FEATURES]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "with timed(\"Splitting train/val\"):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y if y.nunique() > 1 else None\n",
    "    )\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Preprocess + model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "stage(\"Stage 3/7 — Preprocess + model pipeline\")\n",
    "\n",
    "# Everything is numeric (1/2/3 + engineered totals), so keep it simple:\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Fit\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "stage(\"Stage 4/7 — Train\")\n",
    "\n",
    "with timed(\"Fitting model\"):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Evaluate\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "stage(\"Stage 5/7 — Evaluate\")\n",
    "\n",
    "with timed(\"Predicting on validation set\"):\n",
    "    pred = clf.predict(X_val)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_val, pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_val, pred))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Save\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "stage(\"Stage 6/7 — Save model + schema\")\n",
    "\n",
    "MODEL_OUT = \"questionnaire_model.joblib\"\n",
    "SCHEMA_OUT = \"questionnaire_schema.joblib\"\n",
    "\n",
    "with timed(f\"Saving model to {MODEL_OUT}\"):\n",
    "    joblib.dump(clf, MODEL_OUT)\n",
    "\n",
    "# Save the exact feature order your FastAPI must send\n",
    "schema = {\"features\": ALL_FEATURES}\n",
    "joblib.dump(schema, SCHEMA_OUT)\n",
    "\n",
    "print(f\"✅ Saved: {MODEL_OUT}\")\n",
    "print(f\"✅ Saved: {SCHEMA_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Stage 1/6 — Load dataset + verify columns\n",
      "================================================================================\n",
      "▶ Reading data_2000_raw.csv ...\n",
      "✅ Done in 0.05s\n",
      "Rows: 2000\n",
      "Target distribution:\n",
      "risk\n",
      "LOW       1006\n",
      "MEDIUM     664\n",
      "HIGH       330\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Stage 2/6 — Train/validation split\n",
      "================================================================================\n",
      "▶ Splitting train/val ...\n",
      "✅ Done in 0.01s\n",
      "Train size: 1600\n",
      "Val size: 400\n",
      "\n",
      "================================================================================\n",
      "Stage 3/6 — Preprocess + model pipeline\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Stage 4/6 — Train\n",
      "================================================================================\n",
      "▶ Fitting model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 400 out of 400 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done in 0.64s\n",
      "\n",
      "================================================================================\n",
      "Stage 5/6 — Evaluate\n",
      "================================================================================\n",
      "▶ Predicting on validation set ...\n",
      "✅ Done in 0.08s\n",
      "Confusion matrix:\n",
      "[[ 49   0  17]\n",
      " [  0 195   6]\n",
      " [  2   7 124]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.96      0.74      0.84        66\n",
      "         LOW       0.97      0.97      0.97       201\n",
      "      MEDIUM       0.84      0.93      0.89       133\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.92      0.88      0.90       400\n",
      "weighted avg       0.92      0.92      0.92       400\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Stage 6/6 — Save model + schema\n",
      "================================================================================\n",
      "▶ Saving model to questionnaire_model.joblib ...\n",
      "✅ Done in 0.14s\n",
      "▶ Saving schema to questionnaire_schema.joblib ...\n",
      "✅ Done in 0.00s\n",
      "✅ Saved: questionnaire_model.joblib\n",
      "✅ Saved: questionnaire_schema.joblib\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def stage(msg: str):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(msg)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def timed(msg: str):\n",
    "    class _T:\n",
    "        def __enter__(self):\n",
    "            self.t0 = time.time()\n",
    "            print(f\"▶ {msg} ...\")\n",
    "            return self\n",
    "        def __exit__(self, exc_type, exc, tb):\n",
    "            dt = time.time() - self.t0\n",
    "            if exc_type is None:\n",
    "                print(f\"✅ Done in {dt:.2f}s\")\n",
    "            else:\n",
    "                print(f\"❌ Failed after {dt:.2f}s\")\n",
    "    return _T()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config\n",
    "# -----------------------------------------------------------------------------\n",
    "DATA_PATH = \"data_2000_raw.csv\"  # <-- change to your csv file\n",
    "TARGET_COL = \"risk\"              # LOW / MEDIUM / HIGH\n",
    "\n",
    "FEATURES = [\n",
    "    \"A1_1_PEIS\",\"A1_2_FAULT_DISTANCE\",\"A1_3_SEISMIC_SOURCE_TYPE\",\"A1_4_LIQUEFACTION\",\n",
    "    \"A2_1_BASIC_WIND_SPEED\",\"A2_2_BUILDING_VICINITY\",\"A3_1_SLOPE\",\"A3_2_ELEVATION\",\n",
    "    \"A3_3_DISTANCE_TO_RIVERS_AND_SEAS\",\"A3_4_SURFACE_RUNOFF\",\"A3_5_BASE_HEIGHT\",\"A3_6_DRAINAGE_SYSTEM\",\n",
    "\n",
    "    \"B1_1_AESTHETIC_THEME\",\"B1_2_STYLE_UNIQUE\",\"B1_3_STYLE_TYPICAL\",\"B1_4_CITYSCAPE_INTEGRATION\",\n",
    "    \"B2_1_AGE_OF_BUILDING\",\"B2_2_PAST_RELEVANCE\",\"B2_3_GEO_IMPACT\",\"B2_4_CULTURAL_HERITAGE_TIE\",\n",
    "    \"B2_5_MESSAGE_WORTH_PRESERVING\",\"B3_1_NO_INITIATIVES\",\"B3_2_PROMINENT_SUPPORT\",\n",
    "    \"B3_3_IMPORTANCE_DAILY_LIFE\",\"B3_4_NO_PROMOTION\",\"B4_1_TOURIST_MUST_SEE\",\"B4_2_TOURISM_CONTRIBUTION\",\n",
    "    \"B4_3_VISITED_FOR_GOODS\",\"B4_4_CURRENT_USE_ADOPTS_NEEDS\",\n",
    "\n",
    "    \"C1_1_CODE_YEAR_BUILT\",\"C1_2_PLAN_IRREGULARITY\",\"C1_3_VERTICAL_IRREGULARITY\",\"C1_4_BUILDING_PROXIMITY\",\n",
    "    \"C1_5_NUMBER_OF_STOREYS\",\"C1_6_STRUCT_SYSTEM_MATERIAL\",\"C1_7_NUMBER_OF_BAYS\",\"C1_8_COLUMN_SPACING\",\n",
    "    \"C1_9_BUILDING_ENCLOSURE\",\"C1_10_WALL_MATERIAL\",\"C1_11_FRAMING_TYPE\",\"C1_12_FLOORING_MATERIAL\",\n",
    "    \"C2_1_CRACK_WIDTH\",\"C2_2_UNEVEN_SETTLEMENT\",\"C2_3_BEAM_COLUMN_DEFORMATION\",\"C2_4_FINISHING_DETERIORATION\",\n",
    "    \"C2_5_MEMBER_DECAY\",\"C2_6_ADDITIONAL_LOADS\",\"C3_1_ROOF_DESIGN\",\"C3_2_ROOF_SLOPE\",\"C3_3_ROOFING_MATERIAL\",\n",
    "    \"C4_1_ROOF_FASTENERS\",\"C4_2_FASTENER_SPACING\",\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load + verify\n",
    "# -----------------------------------------------------------------------------\n",
    "stage(\"Stage 1/6 — Load dataset + verify columns\")\n",
    "\n",
    "with timed(f\"Reading {DATA_PATH}\"):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "missing = [c for c in FEATURES + [TARGET_COL] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "df = df[FEATURES + [TARGET_COL]].copy()\n",
    "\n",
    "# Ensure numeric features\n",
    "for c in FEATURES:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Target distribution:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Split\n",
    "# -----------------------------------------------------------------------------\n",
    "stage(\"Stage 2/6 — Train/validation split\")\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "with timed(\"Splitting train/val\"):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Preprocess + model\n",
    "# -----------------------------------------------------------------------------\n",
    "stage(\"Stage 3/6 — Preprocess + model pipeline\")\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"scaler\", StandardScaler()),  # OK even if RF doesn't need it\n",
    "])\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Fit\n",
    "# -----------------------------------------------------------------------------\n",
    "stage(\"Stage 4/6 — Train\")\n",
    "\n",
    "with timed(\"Fitting model\"):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Evaluate\n",
    "# -----------------------------------------------------------------------------\n",
    "stage(\"Stage 5/6 — Evaluate\")\n",
    "\n",
    "with timed(\"Predicting on validation set\"):\n",
    "    pred = clf.predict(X_val)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_val, pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_val, pred))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Save\n",
    "# -----------------------------------------------------------------------------\n",
    "stage(\"Stage 6/6 — Save model + schema\")\n",
    "\n",
    "MODEL_OUT = \"questionnaire_model.joblib\"\n",
    "SCHEMA_OUT = \"questionnaire_schema.joblib\"\n",
    "\n",
    "with timed(f\"Saving model to {MODEL_OUT}\"):\n",
    "    joblib.dump(clf, MODEL_OUT)\n",
    "\n",
    "schema = {\"features\": FEATURES, \"target\": TARGET_COL}\n",
    "with timed(f\"Saving schema to {SCHEMA_OUT}\"):\n",
    "    joblib.dump(schema, SCHEMA_OUT)\n",
    "\n",
    "print(f\"✅ Saved: {MODEL_OUT}\")\n",
    "print(f\"✅ Saved: {SCHEMA_OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
